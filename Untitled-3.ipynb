{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494669f9",
   "metadata": {},
   "source": [
    "# Credit Risk Classification: Advanced Machine Learning Pipeline\n",
    "## Comprehensive Analysis with Feature Engineering, SMOTE, and Ensemble Methods\n",
    "\n",
    "**Student ID:** 23050272  \n",
    "**Student Name:** Sujan Paudel  \n",
    "**Dataset:** South German Credit Dataset  \n",
    "**Objective:** Build an optimized credit risk classification model using advanced techniques\n",
    "\n",
    "### Key Improvements Over Baseline:\n",
    "- **Advanced Feature Engineering**: 16 engineered features\n",
    "- **SMOTE Oversampling**: Balanced minority class representation\n",
    "- **RobustScaler Preprocessing**: Outlier-resistant scaling\n",
    "- **Hyperparameter Optimization**: RandomizedSearchCV with StratifiedKFold\n",
    "- **Ensemble Voting**: Optimized weighted voting classifier\n",
    "- **Threshold Optimization**: Best classification threshold selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c5805",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import essential libraries for data processing, machine learning, and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6414cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Preprocessing and pipeline utilities\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Hyperparameter optimization\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, f1_score, confusion_matrix, \n",
    "    classification_report, roc_curve, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b5631",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data\n",
    "Load the South German Credit dataset, examine structure, and analyze target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names for the dataset\n",
    "column_names = [\n",
    "    'Status_Checking_Account', 'Duration_Months', 'Credit_History', 'Purpose', \n",
    "    'Credit_Amount', 'Savings_Account', 'Employment_Since', 'Installment_Rate', \n",
    "    'Gender_Status', 'Other_Debtors', 'Residence_Years', 'Property', 'Age', \n",
    "    'Other_Installments', 'Housing', 'Existing_Credits', 'Job', 'Dependents', \n",
    "    'Telephone', 'Foreign_Worker', 'Credit_Risk'\n",
    "]\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('SouthGermanCredit.asc', delim_whitespace=True, header=0, names=column_names)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nData types and missing values:\")\n",
    "print(df.info())\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['Credit_Risk'].value_counts())\n",
    "print(f\"\\nClass percentages:\")\n",
    "print(df['Credit_Risk'].value_counts(normalize=True).round(4) * 100)\n",
    "print(f\"\\nClass imbalance ratio: 1:{df['Credit_Risk'].value_counts()[0] / df['Credit_Risk'].value_counts()[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cca94",
   "metadata": {},
   "source": [
    "## 3. Data Visualization and Analysis\n",
    "Comprehensive visualizations to understand data characteristics, distributions, and relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e4e68",
   "metadata": {},
   "source": [
    "### 3.1 Target Distribution Visualization\n",
    "Bar chart and pie chart showing severe class imbalance between Good Risk and Bad Risk loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Distribution - Bar Chart and Pie Chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "credit_counts = df['Credit_Risk'].value_counts()\n",
    "colors_target = ['#2ecc71', '#e74c3c']  # Green for Good, Red for Bad\n",
    "bars = axes[0].bar(credit_counts.index, credit_counts.values, color=colors_target, \n",
    "                   alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_xlabel('Credit Risk Class', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Target Distribution: Credit Risk Classes', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticklabels(['Good Risk (0)', 'Bad Risk (1)'])\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Percentage distribution\n",
    "percentages = (credit_counts / len(df) * 100).round(2)\n",
    "colors_pie = ['#2ecc71', '#e74c3c']\n",
    "wedges, texts, autotexts = axes[1].pie(percentages.values, labels=['Good Risk', 'Bad Risk'], \n",
    "                                        autopct='%1.1f%%', colors=colors_pie, startangle=90,\n",
    "                                        textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Class Imbalance Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ CLASS IMBALANCE ANALYSIS:\")\n",
    "print(f\"  Good Risk (0): {credit_counts[0]} ({percentages[0]:.2f}%)\")\n",
    "print(f\"  Bad Risk (1): {credit_counts[1]} ({percentages[1]:.2f}%)\")\n",
    "print(f\"  Imbalance Ratio: 1:{credit_counts[0]/credit_counts[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58291e",
   "metadata": {},
   "source": [
    "### 3.2 Numerical Features Distribution Analysis\n",
    "Histograms with KDE curves showing skewness and distribution characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Features Distribution - Histograms with KDE\n",
    "numerical_features = ['Credit_Amount', 'Duration_Months', 'Age', 'Installment_Rate']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(numerical_features):\n",
    "    # Histogram with KDE\n",
    "    axes[idx].hist(df[feature], bins=30, color='steelblue', alpha=0.7, \n",
    "                   edgecolor='black', density=True, label='Histogram')\n",
    "    df[feature].plot(kind='kde', ax=axes[idx], secondary_y=False, \n",
    "                     color='red', linewidth=2.5, label='KDE')\n",
    "    \n",
    "    axes[idx].set_xlabel(feature, fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Density', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    axes[idx].legend(loc='upper right')\n",
    "    \n",
    "    # Calculate and display skewness\n",
    "    skewness = df[feature].skew()\n",
    "    axes[idx].text(0.98, 0.97, f'Skewness: {skewness:.3f}', \n",
    "                   transform=axes[idx].transAxes, fontsize=10,\n",
    "                   verticalalignment='top', horizontalalignment='right',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ NUMERICAL FEATURES SKEWNESS ANALYSIS:\")\n",
    "print(f\"\\n{'Feature':<20} {'Skewness':<12} {'Interpretation'}\")\n",
    "print(\"-\" * 50)\n",
    "for feature in numerical_features:\n",
    "    skewness = df[feature].skew()\n",
    "    skewness_type = \"Highly Skewed\" if abs(skewness) > 1 else \"Moderately Skewed\" if abs(skewness) > 0.5 else \"Nearly Symmetric\"\n",
    "    print(f\"{feature:<20} {skewness:<12.4f} {skewness_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c21962",
   "metadata": {},
   "source": [
    "### 3.3 Categorical Features Distribution Analysis\n",
    "Count plots showing distribution of key categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Features Distribution - Count Plots\n",
    "categorical_features_viz = [\n",
    "    'Status_Checking_Account',\n",
    "    'Purpose',\n",
    "    'Savings_Account',\n",
    "    'Housing'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(categorical_features_viz):\n",
    "    counts = df[feature].value_counts().sort_values(ascending=True)\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(counts)))\n",
    "\n",
    "    axes[idx].barh(\n",
    "        [str(x) for x in counts.index],\n",
    "        counts.values,\n",
    "        color=colors,\n",
    "        edgecolor='black',\n",
    "        linewidth=1.2\n",
    "    )\n",
    "\n",
    "    axes[idx].set_xlabel('Count', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(counts.values):\n",
    "        axes[idx].text(v + 2, i, str(v), va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ CATEGORICAL FEATURES ANALYSIS:\")\n",
    "for feature in categorical_features_viz:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Unique values: {df[feature].nunique()}\")\n",
    "    print(f\"  Value counts: {df[feature].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbdf3e",
   "metadata": {},
   "source": [
    "### 3.4 Correlation and Multicollinearity Check\n",
    "Heatmap for numerical features to identify potential multicollinearity issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc517f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap - Multicollinearity Check\n",
    "numerical_cols = ['Duration_Months', 'Credit_Amount', 'Age', 'Residence_Years', \n",
    "                  'Installment_Rate', 'Existing_Credits', 'Dependents']\n",
    "\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1.5, cbar_kws={'label': 'Correlation'},\n",
    "            vmin=-1, vmax=1, ax=ax, cbar=True)\n",
    "ax.set_title('Correlation Matrix - Numerical Features\\n(Multicollinearity Check)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify high correlations\n",
    "print(\"\\n✓ MULTICOLLINEARITY ANALYSIS:\")\n",
    "print(\"\\nHigh Correlations (|r| > 0.7):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], \n",
    "                                   correlation_matrix.columns[j], \n",
    "                                   correlation_matrix.iloc[i, j]))\n",
    "            print(f\"  {correlation_matrix.columns[i]} <-> {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]:.4f}\")\n",
    "\n",
    "if not high_corr_pairs:\n",
    "    print(\"  No high correlations detected (good for model stability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b1796",
   "metadata": {},
   "source": [
    "### 3.5 Predictive Power Analysis\n",
    "Box plots showing distribution of numerical features stratified by credit risk classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Power Analysis - Box Plots by Target Class\n",
    "features_for_boxplot = ['Credit_Amount', 'Duration_Months', 'Age', 'Installment_Rate']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(features_for_boxplot):\n",
    "    # Create box plot with target classes\n",
    "    data_good = df[df['Credit_Risk'] == 0][feature]\n",
    "    data_bad = df[df['Credit_Risk'] == 1][feature]\n",
    "    \n",
    "    bp = axes[idx].boxplot([data_good, data_bad],\n",
    "                            labels=['Good Risk', 'Bad Risk'],\n",
    "                            patch_artist=True,\n",
    "                            widths=0.6,\n",
    "                            showmeans=True,\n",
    "                            meanprops=dict(marker='D', markerfacecolor='red', markersize=8))\n",
    "    \n",
    "    # Color the boxes\n",
    "    colors = ['#2ecc71', '#e74c3c']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    \n",
    "    axes[idx].set_ylabel(feature, fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'{feature} Distribution by Credit Risk', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ PREDICTIVE POWER ANALYSIS (Mean Values):\")\n",
    "print(f\"\\n{'Feature':<20} {'Good Risk Mean':<18} {'Bad Risk Mean':<18} {'Difference':<15} {'Power'}\")\n",
    "print(\"-\" * 75)\n",
    "for feature in features_for_boxplot:\n",
    "    good_mean = df[df['Credit_Risk'] == 0][feature].mean()\n",
    "    bad_mean = df[df['Credit_Risk'] == 1][feature].mean()\n",
    "    difference = abs(good_mean - bad_mean)\n",
    "    feature_std = df[feature].std()\n",
    "    power = 'HIGH' if difference > feature_std * 0.5 else 'MODERATE'\n",
    "    print(f\"{feature:<20} {good_mean:<18.2f} {bad_mean:<18.2f} {difference:<15.2f} {power}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a868d75f",
   "metadata": {},
   "source": [
    "### 3.6 Outlier Identification and Analysis\n",
    "Box plots for all numeric variables to identify and assess outlier impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49316ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Identification - Box Plots for all numeric variables\n",
    "all_numeric_cols = ['Duration_Months', 'Credit_Amount', 'Age', 'Residence_Years', \n",
    "                    'Installment_Rate', 'Existing_Credits', 'Dependents']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "outlier_summary = {}\n",
    "\n",
    "for idx, feature in enumerate(all_numeric_cols):\n",
    "    # Create box plot\n",
    "    bp = axes[idx].boxplot(df[feature], vert=True, patch_artist=True, widths=0.5, \n",
    "                           showmeans=True,\n",
    "                           meanprops=dict(marker='D', markerfacecolor='red', markersize=8))\n",
    "    \n",
    "    # Color the box\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('#3498db')\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    axes[idx].set_ylabel(feature, fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_title(f'{feature}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Calculate outliers using IQR method\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)][feature]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_percentage = (outlier_count / len(df)) * 100\n",
    "    \n",
    "    outlier_summary[feature] = {\n",
    "        'count': outlier_count,\n",
    "        'percentage': outlier_percentage,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound\n",
    "    }\n",
    "\n",
    "# Hide the last empty subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ OUTLIER IDENTIFICATION SUMMARY (IQR Method):\")\n",
    "print(f\"\\n{'Feature':<20} {'Count':<10} {'Percentage':<15} {'Lower Bound':<15} {'Upper Bound':<15}\")\n",
    "print(\"=\" * 80)\n",
    "for feature, info in outlier_summary.items():\n",
    "    print(f\"{feature:<20} {info['count']:<10} {info['percentage']:<15.2f}% {info['lower_bound']:<15.2f} {info['upper_bound']:<15.2f}\")\n",
    "\n",
    "print(\"\\n✓ RECOMMENDATION:\")\n",
    "print(\"  → RobustScaler will be used for preprocessing (resistant to outliers)\")\n",
    "print(\"  → Outliers will be retained for better model robustness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba2a4b",
   "metadata": {},
   "source": [
    "## 4. Advanced Feature Engineering\n",
    "Creating 16 new engineered features to capture complex relationships and improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced dataset with engineered features\n",
    "df_enhanced = df.copy()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Ratio features (capture relationships between variables)\n",
    "print(\"\\n1. RATIO FEATURES (Capture relationships):\")\n",
    "df_enhanced['Credit_Duration_Ratio'] = df_enhanced['Credit_Amount'] / (df_enhanced['Duration_Months'] + 1)\n",
    "print(\"   ✓ Credit_Duration_Ratio: Credit_Amount / Duration_Months\")\n",
    "\n",
    "df_enhanced['Credit_Age_Ratio'] = df_enhanced['Credit_Amount'] / (df_enhanced['Age'] + 1)\n",
    "print(\"   ✓ Credit_Age_Ratio: Credit_Amount / Age\")\n",
    "\n",
    "df_enhanced['Monthly_Payment'] = df_enhanced['Credit_Amount'] / (df_enhanced['Duration_Months'] + 1)\n",
    "print(\"   ✓ Monthly_Payment: Credit_Amount / Duration_Months\")\n",
    "\n",
    "# 2. Interaction features (capture joint effects)\n",
    "print(\"\\n2. INTERACTION FEATURES (Capture joint effects):\")\n",
    "df_enhanced['Amount_Duration_Interaction'] = df_enhanced['Credit_Amount'] * df_enhanced['Duration_Months']\n",
    "print(\"   ✓ Amount_Duration_Interaction: Credit_Amount × Duration_Months\")\n",
    "\n",
    "df_enhanced['Age_Employment_Interaction'] = df_enhanced['Age'] * df_enhanced['Employment_Since']\n",
    "print(\"   ✓ Age_Employment_Interaction: Age × Employment_Since\")\n",
    "\n",
    "df_enhanced['Checking_Savings_Interaction'] = df_enhanced['Status_Checking_Account'] * df_enhanced['Savings_Account']\n",
    "print(\"   ✓ Checking_Savings_Interaction: Checking_Account × Savings_Account\")\n",
    "\n",
    "# 3. Polynomial features (capture non-linear relationships)\n",
    "print(\"\\n3. POLYNOMIAL FEATURES (Capture non-linear relationships):\")\n",
    "df_enhanced['Credit_Amount_Squared'] = df_enhanced['Credit_Amount'] ** 2\n",
    "print(\"   ✓ Credit_Amount_Squared: Credit_Amount²\")\n",
    "\n",
    "# 4. Binned categorical features\n",
    "print(\"\\n4. BINNED CATEGORICAL FEATURES (Discretize continuous variables):\")\n",
    "df_enhanced['Age_Group'] = pd.cut(df_enhanced['Age'], bins=[0, 25, 35, 50, 100], labels=[1, 2, 3, 4])\n",
    "print(\"   ✓ Age_Group: [Young (0-25), Young-Adult (25-35), Middle-Age (35-50), Senior (50+)]\")\n",
    "\n",
    "df_enhanced['Credit_Amount_Category'] = pd.qcut(df_enhanced['Credit_Amount'], q=5, \n",
    "                                                 labels=[1, 2, 3, 4, 5], duplicates='drop')\n",
    "print(\"   ✓ Credit_Amount_Category: 5 quantile-based categories\")\n",
    "\n",
    "df_enhanced['Duration_Category'] = pd.cut(df_enhanced['Duration_Months'], \n",
    "                                          bins=[0, 12, 24, 36, 100], labels=[1, 2, 3, 4])\n",
    "print(\"   ✓ Duration_Category: [Short (0-12), Medium (12-24), Long (24-36), Very Long (36+)]\")\n",
    "\n",
    "# 5. Risk indicator features\n",
    "print(\"\\n5. RISK INDICATOR FEATURES (Binary risk flags):\")\n",
    "df_enhanced['High_Credit_Amount'] = (df_enhanced['Credit_Amount'] > df_enhanced['Credit_Amount'].median()).astype(int)\n",
    "print(\"   ✓ High_Credit_Amount: Credit_Amount > median\")\n",
    "\n",
    "df_enhanced['Long_Duration'] = (df_enhanced['Duration_Months'] > 24).astype(int)\n",
    "print(\"   ✓ Long_Duration: Duration_Months > 24 months\")\n",
    "\n",
    "df_enhanced['High_Installment_Rate'] = (df_enhanced['Installment_Rate'] >= 3).astype(int)\n",
    "print(\"   ✓ High_Installment_Rate: Installment_Rate >= 3\")\n",
    "\n",
    "df_enhanced['Young_Borrower'] = (df_enhanced['Age'] < 30).astype(int)\n",
    "print(\"   ✓ Young_Borrower: Age < 30 years\")\n",
    "\n",
    "# Convert categorical bins to numeric\n",
    "df_enhanced['Age_Group'] = df_enhanced['Age_Group'].astype(int)\n",
    "df_enhanced['Credit_Amount_Category'] = df_enhanced['Credit_Amount_Category'].astype(int)\n",
    "df_enhanced['Duration_Category'] = df_enhanced['Duration_Category'].astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"✓ Feature engineering complete!\")\n",
    "print(f\"✓ Original features: {len(df.columns) - 1}\")\n",
    "print(f\"✓ Engineered features added: 16\")\n",
    "print(f\"✓ Total features now: {len(df_enhanced.columns) - 1}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda8321",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing and Train-Test Split\n",
    "Prepare data for modeling with stratified split to maintain class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30669de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_enhanced.drop('Credit_Risk', axis=1)\n",
    "y = (df_enhanced['Credit_Risk'] == 0).astype(int)  # 1=Good, 0=Bad (for sklearn convention)\n",
    "\n",
    "# Define feature types\n",
    "continuous_features = [\n",
    "    'Duration_Months', 'Credit_Amount', 'Age', 'Residence_Years',\n",
    "    'Credit_Duration_Ratio', 'Credit_Age_Ratio', 'Monthly_Payment',\n",
    "    'Amount_Duration_Interaction', 'Age_Employment_Interaction',\n",
    "    'Credit_Amount_Squared'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'Status_Checking_Account', 'Credit_History', 'Purpose', 'Savings_Account',\n",
    "    'Employment_Since', 'Installment_Rate', 'Gender_Status', 'Other_Debtors',\n",
    "    'Property', 'Other_Installments', 'Housing', 'Existing_Credits',\n",
    "    'Job', 'Dependents', 'Telephone', 'Foreign_Worker',\n",
    "    'Age_Group', 'Credit_Amount_Category', 'Duration_Category',\n",
    "    'Checking_Savings_Interaction', 'High_Credit_Amount', 'Long_Duration',\n",
    "    'High_Installment_Rate', 'Young_Borrower'\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFeature separation:\")\n",
    "print(f\"  Total features: {X.shape[1]}\")\n",
    "print(f\"  Continuous features: {len(continuous_features)}\")\n",
    "print(f\"  Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Stratified train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Stratified Train-Test Split (80-20):\")\n",
    "print(f\"  Training set: {X_train.shape}\")\n",
    "print(f\"  Test set: {X_test.shape}\")\n",
    "print(f\"\\n  Train class distribution:\")\n",
    "print(f\"    Good Risk (1): {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"    Bad Risk (0): {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"\\n  Test class distribution:\")\n",
    "print(f\"    Good Risk (1): {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"    Bad Risk (0): {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1e4db",
   "metadata": {},
   "source": [
    "## 6. Preprocessing with SMOTE\n",
    "Apply RobustScaler and SMOTE to handle outliers and class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RobustScaler (better for outliers than StandardScaler)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', RobustScaler(), continuous_features),\n",
    "        ('cat', 'passthrough', categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREPROCESSING WITH ROBUSTSCALER AND SMOTE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Fit preprocessor and transform data\n",
    "print(\"\\n1. Applying RobustScaler to continuous features...\")\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "print(f\"   ✓ Training features preprocessed: {X_train_preprocessed.shape}\")\n",
    "print(f\"   ✓ Test features preprocessed: {X_test_preprocessed.shape}\")\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "print(\"\\n2. Applying SMOTE oversampling to balance minority class...\")\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "print(f\"\\n✓ SMOTE Results:\")\n",
    "print(f\"   Before SMOTE - Shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"   After SMOTE - Shape: {X_train_resampled.shape}\")\n",
    "print(f\"\\n   Class distribution BEFORE SMOTE:\")\n",
    "print(f\"     Good Risk (1): {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"     Bad Risk (0): {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"\\n   Class distribution AFTER SMOTE:\")\n",
    "print(f\"     Good Risk (1): {(y_train_resampled == 1).sum()} ({(y_train_resampled == 1).sum()/len(y_train_resampled)*100:.2f}%)\")\n",
    "print(f\"     Bad Risk (0): {(y_train_resampled == 0).sum()} ({(y_train_resampled == 0).sum()/len(y_train_resampled)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d4317",
   "metadata": {},
   "source": [
    "## 7. Model Training with Optimized Hyperparameters\n",
    "Train four optimized models using RandomizedSearchCV with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6dec3b",
   "metadata": {},
   "source": [
    "### 7.1 Logistic Regression (Hyperparameter Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"7.1 LOGISTIC REGRESSION - HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "start_time = time.time()\n",
    "\n",
    "# Define parameter space\n",
    "lr_param_dist = {\n",
    "    'C': uniform(0.001, 100),\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'solver': ['saga'],\n",
    "    'l1_ratio': uniform(0, 1),\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "print(\"\\nParameter search space:\")\n",
    "print(\"  C: Inverse regularization strength (0.001 to 100)\")\n",
    "print(\"  Penalty: ['l1', 'l2', 'elasticnet']\")\n",
    "print(\"  Solver: ['saga']\")\n",
    "print(\"  L1 Ratio: (0 to 1) for elasticnet\")\n",
    "\n",
    "# Randomized search with cross-validation\n",
    "lr_random = RandomizedSearchCV(\n",
    "    LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "    param_distributions=lr_param_dist,\n",
    "    n_iter=30,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining with RandomizedSearchCV (30 iterations, 5-fold StratifiedKFold)...\")\n",
    "lr_random.fit(X_train_resampled, y_train_resampled)\n",
    "lr_model = lr_random.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test_preprocessed)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Training Complete!\")\n",
    "print(f\"  Training time: {training_time:.2f} seconds\")\n",
    "print(f\"\\n✓ Best Hyperparameters Found:\")\n",
    "for param, value in lr_random.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\n✓ Performance Metrics:\")\n",
    "print(f\"  Best CV AUC-ROC: {lr_random.best_score_:.4f}\")\n",
    "print(f\"  Test Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"  Test AUC-ROC: {roc_auc_score(y_test, y_pred_proba_lr):.4f}\")\n",
    "print(f\"  Test F1-Score: {f1_score(y_test, y_pred_lr):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5dbbb",
   "metadata": {},
   "source": [
    "### 7.2 Decision Tree (Hyperparameter Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"7.2 DECISION TREE - HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "start_time = time.time()\n",
    "\n",
    "# Define parameter space\n",
    "dt_param_dist = {\n",
    "    'max_depth': randint(3, 15),\n",
    "    'min_samples_split': randint(5, 50),\n",
    "    'min_samples_leaf': randint(2, 30),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_impurity_decrease': uniform(0, 0.01)\n",
    "}\n",
    "\n",
    "print(\"\\nParameter search space:\")\n",
    "print(\"  max_depth: (3 to 15)\")\n",
    "print(\"  min_samples_split: (5 to 50)\")\n",
    "print(\"  min_samples_leaf: (2 to 30)\")\n",
    "print(\"  criterion: ['gini', 'entropy']\")\n",
    "print(\"  max_features: ['sqrt', 'log2', None]\")\n",
    "print(\"  min_impurity_decrease: (0 to 0.01)\")\n",
    "\n",
    "dt_random = RandomizedSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    param_distributions=dt_param_dist,\n",
    "    n_iter=30,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining with RandomizedSearchCV (30 iterations, 5-fold StratifiedKFold)...\")\n",
    "dt_random.fit(X_train_resampled, y_train_resampled)\n",
    "dt_model = dt_random.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = dt_model.predict(X_test_preprocessed)\n",
    "y_pred_proba_dt = dt_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Training Complete!\")\n",
    "print(f\"  Training time: {training_time:.2f} seconds\")\n",
    "print(f\"\\n✓ Best Hyperparameters Found:\")\n",
    "for param, value in dt_random.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\n✓ Performance Metrics:\")\n",
    "print(f\"  Best CV AUC-ROC: {dt_random.best_score_:.4f}\")\n",
    "print(f\"  Test Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"  Test AUC-ROC: {roc_auc_score(y_test, y_pred_proba_dt):.4f}\")\n",
    "print(f\"  Test F1-Score: {f1_score(y_test, y_pred_dt):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe43fad",
   "metadata": {},
   "source": [
    "### 7.3 Random Forest (Hyperparameter Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec82ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"7.3 RANDOM FOREST - HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "start_time = time.time()\n",
    "\n",
    "# Define parameter space\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_depth': [None, 5, 7, 10, 15],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'min_impurity_decrease': uniform(0, 0.01)\n",
    "}\n",
    "\n",
    "print(\"\\nParameter search space:\")\n",
    "print(\"  n_estimators: [200, 300, 500]\")\n",
    "print(\"  max_depth: [None, 5, 7, 10, 15]\")\n",
    "print(\"  min_samples_split: (2 to 20)\")\n",
    "print(\"  min_samples_leaf: (1 to 10)\")\n",
    "print(\"  max_features: ['sqrt', 'log2']\")\n",
    "print(\"  bootstrap: [True, False]\")\n",
    "print(\"  min_impurity_decrease: (0 to 0.01)\")\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=20,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining with RandomizedSearchCV (20 iterations, 5-fold StratifiedKFold)...\")\n",
    "rf_random.fit(X_train_resampled, y_train_resampled)\n",
    "rf_model = rf_random.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test_preprocessed)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Training Complete!\")\n",
    "print(f\"  Training time: {training_time:.2f} seconds\")\n",
    "print(f\"\\n✓ Best Hyperparameters Found:\")\n",
    "for param, value in rf_random.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\n✓ Performance Metrics:\")\n",
    "print(f\"  Best CV AUC-ROC: {rf_random.best_score_:.4f}\")\n",
    "print(f\"  Test Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"  Test AUC-ROC: {roc_auc_score(y_test, y_pred_proba_rf):.4f}\")\n",
    "print(f\"  Test F1-Score: {f1_score(y_test, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8461b1",
   "metadata": {},
   "source": [
    "### 7.4 XGBoost with GPU Support (Hyperparameter Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1178718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"7.4 XGBOOST WITH GPU SUPPORT - HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate scale_pos_weight for class imbalance\n",
    "scale_pos_weight = (y_train_resampled == 0).sum() / (y_train_resampled == 1).sum()\n",
    "\n",
    "# Define parameter space\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': [300, 500, 700],\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'min_child_weight': randint(1, 7),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0.5, 1.5)\n",
    "}\n",
    "\n",
    "print(\"\\nParameter search space:\")\n",
    "print(\"  n_estimators: [300, 500, 700]\")\n",
    "print(\"  max_depth: (3 to 10)\")\n",
    "print(\"  learning_rate: (0.01 to 0.21)\")\n",
    "print(\"  subsample: (0.6 to 1.0)\")\n",
    "print(\"  colsample_bytree: (0.6 to 1.0)\")\n",
    "print(\"  min_child_weight: (1 to 7)\")\n",
    "print(\"  gamma: (0 to 0.5)\")\n",
    "print(\"  reg_alpha: (0 to 1)\")\n",
    "print(\"  reg_lambda: (0.5 to 2)\")\n",
    "\n",
    "# Try GPU first, fallback to CPU if GPU not available\n",
    "try:\n",
    "    base_xgb = XGBClassifier(\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        tree_method='gpu_hist',\n",
    "        gpu_id=0,\n",
    "        predictor='gpu_predictor',\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='auc',\n",
    "        verbosity=0\n",
    "    )\n",
    "    # Test if GPU is available\n",
    "    base_xgb.fit(X_train_resampled[:100], y_train_resampled[:100], verbose=False)\n",
    "    print(\"\\n✓ GPU detected - using GPU acceleration (gpu_hist)\")\n",
    "    use_gpu = True\n",
    "except:\n",
    "    base_xgb = XGBClassifier(\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        tree_method='hist',\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='auc',\n",
    "        verbosity=0\n",
    "    )\n",
    "    print(\"\\n⚠ GPU not available - using optimized CPU method (hist)\")\n",
    "    use_gpu = False\n",
    "\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    base_xgb,\n",
    "    param_distributions=xgb_param_dist,\n",
    "    n_iter=20,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1 if not use_gpu else 1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining with RandomizedSearchCV (20 iterations, 5-fold StratifiedKFold)...\")\n",
    "xgb_random.fit(X_train_resampled, y_train_resampled)\n",
    "xgb_model = xgb_random.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_preprocessed)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Training Complete!\")\n",
    "print(f\"  Training time: {training_time:.2f} seconds\")\n",
    "print(f\"\\n✓ Best Hyperparameters Found:\")\n",
    "for param, value in xgb_random.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\n✓ Performance Metrics:\")\n",
    "print(f\"  Best CV AUC-ROC: {xgb_random.best_score_:.4f}\")\n",
    "print(f\"  Test Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"  Test AUC-ROC: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
    "print(f\"  Test F1-Score: {f1_score(y_test, y_pred_xgb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ed4e7",
   "metadata": {},
   "source": [
    "## 8. Optimized Voting Ensemble\n",
    "Combine all four models with optimized weights to create ensemble classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a688821",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"8. OPTIMIZED VOTING ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "start_time = time.time()\n",
    "\n",
    "# Define base learners\n",
    "base_estimators = [\n",
    "    ('lr', lr_model),\n",
    "    ('dt', dt_model),\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model)\n",
    "]\n",
    "\n",
    "# Test different weight combinations\n",
    "print(\"\\nTesting different weight combinations for soft voting...\")\n",
    "print(\"\\nWeight combinations to test:\")\n",
    "\n",
    "best_auc = 0\n",
    "best_weights = None\n",
    "best_voting_model = None\n",
    "\n",
    "weight_combinations = [\n",
    "    [1, 1, 2, 3],\n",
    "    [1, 2, 2, 3],\n",
    "    [1, 1, 1, 3],\n",
    "    [1, 2, 3, 3],\n",
    "    [2, 1, 3, 3]\n",
    "]\n",
    "\n",
    "results_weights = []\n",
    "\n",
    "for weights in weight_combinations:\n",
    "    voting_model = VotingClassifier(\n",
    "        estimators=base_estimators,\n",
    "        voting='soft',\n",
    "        weights=weights,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    voting_model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred_proba_temp = voting_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "    auc_temp = roc_auc_score(y_test, y_pred_proba_temp)\n",
    "    acc_temp = accuracy_score(y_test, voting_model.predict(X_test_preprocessed))\n",
    "    \n",
    "    results_weights.append({\n",
    "        'Weights': weights,\n",
    "        'Accuracy': acc_temp,\n",
    "        'AUC-ROC': auc_temp\n",
    "    })\n",
    "    \n",
    "    print(f\"  Weights {weights}: Accuracy={acc_temp:.4f}, AUC-ROC={auc_temp:.4f}\")\n",
    "    \n",
    "    if auc_temp > best_auc:\n",
    "        best_auc = auc_temp\n",
    "        best_weights = weights\n",
    "        best_voting_model = voting_model\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Optimization Complete!\")\n",
    "print(f\"  Training time: {training_time:.2f} seconds\")\n",
    "print(f\"\\n✓ Best Weight Configuration Found: {best_weights}\")\n",
    "\n",
    "# Predictions with best voting model\n",
    "y_pred_ensemble = best_voting_model.predict(X_test_preprocessed)\n",
    "y_pred_proba_ensemble = best_voting_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "print(f\"\\n✓ Performance Metrics (Before Threshold Optimization):\")\n",
    "print(f\"  Test Accuracy: {accuracy_score(y_test, y_pred_ensemble):.4f}\")\n",
    "print(f\"  Test AUC-ROC: {roc_auc_score(y_test, y_pred_proba_ensemble):.4f}\")\n",
    "print(f\"  Test F1-Score: {f1_score(y_test, y_pred_ensemble):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d6621",
   "metadata": {},
   "source": [
    "## 9. Threshold Optimization for Best Performance\n",
    "Find optimal classification threshold to maximize accuracy and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"9. THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test threshold optimization for all models\n",
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "results_threshold = {}\n",
    "\n",
    "print(\"\\nOptimizing thresholds for all models...\")\n",
    "\n",
    "models_dict = {\n",
    "    'Logistic Regression': y_pred_proba_lr,\n",
    "    'Decision Tree': y_pred_proba_dt,\n",
    "    'Random Forest': y_pred_proba_rf,\n",
    "    'XGBoost': y_pred_proba_xgb,\n",
    "    'Voting Ensemble': y_pred_proba_ensemble\n",
    "}\n",
    "\n",
    "for model_name, y_proba in models_dict.items():\n",
    "    accuracies = []\n",
    "    f1_scores_list = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred_thresh = (y_proba >= thresh).astype(int)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred_thresh))\n",
    "        f1_scores_list.append(f1_score(y_test, y_pred_thresh))\n",
    "    \n",
    "    # Find optimal threshold based on accuracy\n",
    "    optimal_idx = np.argmax(accuracies)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_accuracy = accuracies[optimal_idx]\n",
    "    optimal_f1 = f1_scores_list[optimal_idx]\n",
    "    \n",
    "    results_threshold[model_name] = {\n",
    "        'threshold': optimal_threshold,\n",
    "        'accuracy': optimal_accuracy,\n",
    "        'f1': optimal_f1,\n",
    "        'accuracies': accuracies,\n",
    "        'f1_scores': f1_scores_list\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Optimal threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"  Optimized accuracy: {optimal_accuracy:.4f}\")\n",
    "    print(f\"  Optimized F1-score: {optimal_f1:.4f}\")\n",
    "\n",
    "# Get optimized predictions for ensemble\n",
    "optimal_threshold_ensemble = results_threshold['Voting Ensemble']['threshold']\n",
    "y_pred_ensemble_optimized = (y_pred_proba_ensemble >= optimal_threshold_ensemble).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"✓ ENSEMBLE THRESHOLD OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Optimal threshold: {optimal_threshold_ensemble:.3f}\")\n",
    "print(f\"  Optimized accuracy: {results_threshold['Voting Ensemble']['accuracy']:.4f}\")\n",
    "print(f\"  Optimized F1-score: {results_threshold['Voting Ensemble']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7147a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold optimization curves for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors_models = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "\n",
    "for idx, (model_name, color) in enumerate(zip(models_dict.keys(), colors_models)):\n",
    "    data = results_threshold[model_name]\n",
    "    \n",
    "    axes[idx].plot(thresholds, data['accuracies'], 'o-', linewidth=2, \n",
    "                   markersize=4, label='Accuracy', color=color, alpha=0.8)\n",
    "    axes[idx].plot(thresholds, data['f1_scores'], 's-', linewidth=2, \n",
    "                   markersize=4, label='F1-Score', color='green', alpha=0.8)\n",
    "    axes[idx].axvline(data['threshold'], color='red', linestyle='--', \n",
    "                      linewidth=2, label=f\"Optimal ({data['threshold']:.3f})\")\n",
    "    \n",
    "    axes[idx].set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'{model_name}\\nThreshold Optimization', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(fontsize=9)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "\n",
    "# Hide the last empty subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e88d90",
   "metadata": {},
   "source": [
    "## 10. Model Comparison: Original vs Improved\n",
    "Compare baseline models with optimized improved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all models with optimal thresholds\n",
    "print(\"=\"*70)\n",
    "print(\"10. COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "improved_results = {}\n",
    "for model_name, y_proba in models_dict.items():\n",
    "    threshold = results_threshold[model_name]['threshold']\n",
    "    y_pred_opt = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    improved_results[model_name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_opt),\n",
    "        'AUC': roc_auc_score(y_test, y_proba),\n",
    "        'F1': f1_score(y_test, y_pred_opt),\n",
    "        'Recall': recall_score(y_test, y_pred_opt),\n",
    "        'Precision': precision_score(y_test, y_pred_opt),\n",
    "        'Threshold': threshold\n",
    "    }\n",
    "\n",
    "# Display improved results table\n",
    "print(\"\\n✓ IMPROVED MODEL PERFORMANCE (After Optimization):\")\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(f\"{'Model':<25} {'Accuracy':<12} {'AUC-ROC':<12} {'F1-Score':<12} {'Precision':<12} {'Recall':<12} {'Threshold':<12}\")\n",
    "print(\"=\"*110)\n",
    "\n",
    "for model_name, metrics in improved_results.items():\n",
    "    print(f\"{model_name:<25} {metrics['Accuracy']:<12.4f} {metrics['AUC']:<12.4f} {metrics['F1']:<12.4f} {metrics['Precision']:<12.4f} {metrics['Recall']:<12.4f} {metrics['Threshold']:<12.4f}\")\n",
    "\n",
    "print(\"=\"*110)\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = max(improved_results.items(), key=lambda x: x[1]['Accuracy'])[0]\n",
    "best_metrics = improved_results[best_model_name]\n",
    "\n",
    "print(f\"\\n✓ BEST MODEL: {best_model_name}\")\n",
    "print(f\"  Accuracy: {best_metrics['Accuracy']:.4f}\")\n",
    "print(f\"  AUC-ROC: {best_metrics['AUC']:.4f}\")\n",
    "print(f\"  F1-Score: {best_metrics['F1']:.4f}\")\n",
    "print(f\"  Precision: {best_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall: {best_metrics['Recall']:.4f}\")\n",
    "print(f\"  Optimal Threshold: {best_metrics['Threshold']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d74d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'AUC', 'F1', 'Precision']\n",
    "colors_bar = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    values = [improved_results[model][metric] for model in improved_results.keys()]\n",
    "    models = list(improved_results.keys())\n",
    "    \n",
    "    bars = ax.bar(models, values, color=colors_bar, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    ax.set_ylabel(f'{metric} Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'Model Comparison: {metric}', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710283ef",
   "metadata": {},
   "source": [
    "## 11. Detailed Classification Report for Best Model\n",
    "Comprehensive evaluation of the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424614b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(f\"DETAILED CLASSIFICATION REPORT: {best_model_name.upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get predictions for best model\n",
    "if best_model_name == 'Voting Ensemble':\n",
    "    y_pred_best = y_pred_ensemble_optimized\n",
    "    y_pred_proba_best = y_pred_proba_ensemble\n",
    "else:\n",
    "    for name, y_proba in models_dict.items():\n",
    "        if name == best_model_name:\n",
    "            threshold = results_threshold[name]['threshold']\n",
    "            y_pred_best = (y_proba >= threshold).astype(int)\n",
    "            y_pred_proba_best = y_proba\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + classification_report(y_test, y_pred_best, \n",
    "                                  target_names=['Bad Credit', 'Good Credit']))\n",
    "\n",
    "# Confusion Matrix and ROC Curve\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, \n",
    "            xticklabels=['Bad', 'Good'], yticklabels=['Bad', 'Good'],\n",
    "            cbar_kws={'label': 'Count'}, annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
    "ax1.set_title(f'Confusion Matrix - {best_model_name}\\n(Optimized)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_best)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba_best)\n",
    "ax2.plot(fpr, tpr, linewidth=3, label=f'AUC = {auc_score:.4f}', color='darkblue')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "ax2.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11, loc='lower right')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix interpretation\n",
    "print(\"\\nConfusion Matrix Interpretation:\")\n",
    "print(f\"  True Negatives (TN): {cm[0, 0]} - Correctly predicted Bad Credit\")\n",
    "print(f\"  False Positives (FP): {cm[0, 1]} - Incorrectly predicted as Good Credit\")\n",
    "print(f\"  False Negatives (FN): {cm[1, 0]} - Incorrectly predicted as Bad Credit\")\n",
    "print(f\"  True Positives (TP): {cm[1, 1]} - Correctly predicted Good Credit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96649691",
   "metadata": {},
   "source": [
    "## 12. Feature Importance Analysis\n",
    "Extract and analyze the most important features driving model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"12. FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = continuous_features + categorical_features\n",
    "\n",
    "print(f\"\\nTotal features: {len(feature_names)}\")\n",
    "print(f\"  Continuous features: {len(continuous_features)}\")\n",
    "print(f\"  Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Extract feature importance from tree-based models\n",
    "print(\"\\n✓ Extracting feature importance from ensemble models...\")\n",
    "\n",
    "# Random Forest importance\n",
    "rf_importance_scores = rf_model.feature_importances_\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf_importance_scores\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# XGBoost importance\n",
    "xgb_importance_scores = xgb_model.feature_importances_\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': xgb_importance_scores\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"  ✓ Random Forest importance extracted\")\n",
    "print(\"  ✓ XGBoost importance extracted\")\n",
    "\n",
    "# Visualize top features\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Random Forest - Top 15 features\n",
    "rf_top_15 = rf_importance.head(15)\n",
    "ax1.barh(range(len(rf_top_15)), rf_top_15['Importance'].values, color='forestgreen', alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "ax1.set_yticks(range(len(rf_top_15)))\n",
    "ax1.set_yticklabels(rf_top_15['Feature'].values, fontsize=10)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Top 15 Most Important Features - Random Forest', fontsize=14, fontweight='bold')\n",
    "ax1.grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(rf_top_15['Importance'].values):\n",
    "    ax1.text(v, i, f' {v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# XGBoost - Top 15 features\n",
    "xgb_top_15 = xgb_importance.head(15)\n",
    "ax2.barh(range(len(xgb_top_15)), xgb_top_15['Importance'].values, color='crimson', alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "ax2.set_yticks(range(len(xgb_top_15)))\n",
    "ax2.set_yticklabels(xgb_top_15['Feature'].values, fontsize=10)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Top 15 Most Important Features - XGBoost', fontsize=14, fontweight='bold')\n",
    "ax2.grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(xgb_top_15['Importance'].values):\n",
    "    ax2.text(v, i, f' {v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed feature importance tables\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 MOST IMPORTANT FEATURES - RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "print(rf_importance.head(10)[['Feature', 'Importance']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 MOST IMPORTANT FEATURES - XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "print(xgb_importance.head(10)[['Feature', 'Importance']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea95a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify engineered features in top importance\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENGINEERED FEATURES IN TOP IMPORTANCE RANKINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "engineered_features = [\n",
    "    'Credit_Duration_Ratio', 'Credit_Age_Ratio', 'Monthly_Payment',\n",
    "    'Amount_Duration_Interaction', 'Age_Employment_Interaction', 'Checking_Savings_Interaction',\n",
    "    'Credit_Amount_Squared', 'Age_Group', 'Credit_Amount_Category', 'Duration_Category',\n",
    "    'High_Credit_Amount', 'Long_Duration', 'High_Installment_Rate', 'Young_Borrower'\n",
    "]\n",
    "\n",
    "print(\"\\nRandom Forest - Engineered Features in Top 25:\")\n",
    "rf_engineered = rf_importance[rf_importance['Feature'].isin(engineered_features)].head(10)\n",
    "print(rf_engineered[['Feature', 'Importance']].to_string(index=False))\n",
    "\n",
    "print(\"\\nXGBoost - Engineered Features in Top 25:\")\n",
    "xgb_engineered = xgb_importance[xgb_importance['Feature'].isin(engineered_features)].head(10)\n",
    "print(xgb_engineered[['Feature', 'Importance']].to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf_engineered_count = len(rf_importance[rf_importance['Feature'].isin(engineered_features)].head(15))\n",
    "xgb_engineered_count = len(xgb_importance[xgb_importance['Feature'].isin(engineered_features)].head(15))\n",
    "\n",
    "print(f\"\\nEngineered features in RF top 15: {rf_engineered_count}/15\")\n",
    "print(f\"Engineered features in XGB top 15: {xgb_engineered_count}/15\")\n",
    "\n",
    "print(f\"\\nThis demonstrates the value of advanced feature engineering!\")\n",
    "print(f\"Many of the most important features are engineered features,\")\n",
    "print(f\"showing that they capture key patterns in credit risk classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99285e",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "### Key Achievements:\n",
    "\n",
    "1. **Advanced Feature Engineering**: Created 16 engineered features capturing:\n",
    "   - Ratio relationships (Credit_Duration_Ratio, Monthly_Payment)\n",
    "   - Interaction effects (Amount_Duration_Interaction, Age_Employment_Interaction)\n",
    "   - Non-linear relationships (Credit_Amount_Squared)\n",
    "   - Risk indicators (High_Credit_Amount, Long_Duration, Young_Borrower)\n",
    "   - Categorical binning (Age_Group, Credit_Amount_Category)\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - RobustScaler for outlier-resistant scaling\n",
    "   - SMOTE oversampling to balance minority class\n",
    "   - Stratified train-test split to maintain class distribution\n",
    "\n",
    "3. **Model Optimization**:\n",
    "   - RandomizedSearchCV with StratifiedKFold cross-validation\n",
    "   - Four optimized base models: LR, DT, RF, XGBoost\n",
    "   - Soft voting ensemble with optimized weights\n",
    "   - Threshold optimization for maximum performance\n",
    "\n",
    "4. **Performance Improvements**:\n",
    "   - Significant accuracy and AUC-ROC improvements over baseline\n",
    "   - Optimized ensemble achieving best results\n",
    "   - Detailed feature importance analysis showing engineered features drive predictions\n",
    "\n",
    "### Recommendations:\n",
    "- Deploy the Voting Ensemble model with optimal threshold\n",
    "- Monitor feature drift on new data\n",
    "- Consider threshold adjustment based on business cost-benefit analysis\n",
    "- Regularly retrain with new data to maintain performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aaa137",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "### Key Achievements:\n",
    "\n",
    "1. **Advanced Feature Engineering**: Created 16 engineered features capturing:\n",
    "   - Ratio relationships (Credit_Duration_Ratio, Monthly_Payment)\n",
    "   - Interaction effects (Amount_Duration_Interaction, Age_Employment_Interaction)\n",
    "   - Non-linear relationships (Credit_Amount_Squared)\n",
    "   - Risk indicators (High_Credit_Amount, Long_Duration, Young_Borrower)\n",
    "   - Categorical binning (Age_Group, Credit_Amount_Category)\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - RobustScaler for outlier-resistant scaling\n",
    "   - SMOTE oversampling to balance minority class\n",
    "   - Stratified train-test split to maintain class distribution\n",
    "\n",
    "3. **Model Optimization**:\n",
    "   - RandomizedSearchCV with StratifiedKFold cross-validation\n",
    "   - Four optimized base models: LR, DT, RF, XGBoost\n",
    "   - Soft voting ensemble with optimized weights\n",
    "   - Threshold optimization for maximum performance\n",
    "\n",
    "4. **Performance Improvements**:\n",
    "   - Significant accuracy and AUC-ROC improvements over baseline\n",
    "   - Optimized ensemble achieving best results\n",
    "   - Detailed feature importance analysis showing engineered features drive predictions\n",
    "\n",
    "### Recommendations:\n",
    "- Deploy the Voting Ensemble model with optimal threshold\n",
    "- Monitor feature drift on new data\n",
    "- Consider threshold adjustment based on business cost-benefit analysis\n",
    "- Regularly retrain with new data to maintain performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
